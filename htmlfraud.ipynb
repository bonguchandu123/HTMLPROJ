{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPUh0idOTUdLmagh4JCl0u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bonguchandu123/HTMLPROJ/blob/main/htmlfraud.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tENKxXcQAOUy",
        "outputId": "94f3a4bf-150f-46f0-ddb6-4a8cffd8c8b0",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (0.116.1)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.12/dist-packages (0.35.0)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.3.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Collecting pennylane\n",
            "  Downloading pennylane-0.42.3-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (0.47.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from fastapi) (2.11.7)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (4.14.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (8.2.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from pennylane) (3.5)\n",
            "Collecting rustworkx>=0.14.0 (from pennylane)\n",
            "  Downloading rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.12/dist-packages (from pennylane) (1.8.0)\n",
            "Collecting appdirs (from pennylane)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting autoray<0.8,>=0.6.11 (from pennylane)\n",
            "  Downloading autoray-0.7.2-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from pennylane) (5.5.2)\n",
            "Collecting pennylane-lightning>=0.42 (from pennylane)\n",
            "  Downloading pennylane_lightning-0.42.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from pennylane) (2.32.4)\n",
            "Requirement already satisfied: tomlkit in /usr/local/lib/python3.12/dist-packages (from pennylane) (0.13.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from pennylane) (25.0)\n",
            "Collecting diastatic-malt (from pennylane)\n",
            "  Downloading diastatic_malt-2.15.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Collecting scipy-openblas32>=0.3.26 (from pennylane-lightning>=0.42->pennylane)\n",
            "  Downloading scipy_openblas32-0.3.30.0.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.1/57.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from starlette<0.48.0,>=0.40.0->fastapi) (4.10.0)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (1.6.3)\n",
            "Requirement already satisfied: gast in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (0.6.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (3.1.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (2025.8.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse->diastatic-malt->pennylane) (0.45.1)\n",
            "Downloading pyngrok-7.3.0-py3-none-any.whl (25 kB)\n",
            "Downloading pennylane-0.42.3-py3-none-any.whl (4.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autoray-0.7.2-py3-none-any.whl (930 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m930.8/930.8 kB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pennylane_lightning-0.42.0-cp312-cp312-manylinux_2_28_x86_64.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading diastatic_malt-2.15.2-py3-none-any.whl (167 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m167.9/167.9 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy_openblas32-0.3.30.0.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m102.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: appdirs, scipy-openblas32, rustworkx, pyngrok, autoray, diastatic-malt, pennylane-lightning, pennylane\n",
            "Successfully installed appdirs-1.4.4 autoray-0.7.2 diastatic-malt-2.15.2 pennylane-0.42.3 pennylane-lightning-0.42.0 pyngrok-7.3.0 rustworkx-0.17.1 scipy-openblas32-0.3.30.0.2\n"
          ]
        }
      ],
      "source": [
        "!pip install fastapi uvicorn pyngrok scikit-learn pandas pennylane matplotlib\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install uvicorn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWiaQHQYBHnY",
        "outputId": "71c2ea19-1ee5-44a7-a1ed-56335ad40679",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.12/dist-packages (0.35.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (8.2.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken 31e3W5Dm2p41ltYXjyGN5qJMUOc_3Pj1mN7dntp2JXHNceREo\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9Evp9PfDlQm",
        "outputId": "82656e12-8a1a-4da0-e0ec-5004d63aa768"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, File, UploadFile, HTTPException\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import io\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, f1_score, classification_report, confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pennylane as qml\n",
        "from pennylane import numpy as qnp\n",
        "import pickle\n",
        "import json\n",
        "from typing import List, Optional\n",
        "import time\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "from pyngrok import ngrok\n",
        "\n",
        "\n",
        "app = FastAPI(title=\"Quantum Fraud Detection API\", version=\"1.0.0\")\n",
        "\n",
        "# CORS middleware\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# Global variables to store models\n",
        "quantum_model = None\n",
        "scaler_quantum = None\n",
        "classical_models = {}\n",
        "scaler_classical = None\n",
        "\n",
        "# Quantum setup\n",
        "n_qubits = 4\n",
        "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "\n",
        "def feature_map(x):\n",
        "    for i in range(n_qubits):\n",
        "        qml.RY(x[i], wires=i)\n",
        "    for i in range(n_qubits-1):\n",
        "        qml.CNOT(wires=[i, i+1])\n",
        "\n",
        "def ansatz(weights):\n",
        "    for i in range(n_qubits):\n",
        "        qml.RY(weights[i], wires=i)\n",
        "        qml.RZ(weights[i+n_qubits], wires=i)\n",
        "\n",
        "@qml.qnode(dev, interface=\"autograd\")\n",
        "def circuit(x, weights):\n",
        "    feature_map(x)\n",
        "    ansatz(weights)\n",
        "    return qml.expval(qml.PauliZ(0))\n",
        "\n",
        "def predict_batch(W, Xb):\n",
        "    outs = []\n",
        "    for x in Xb:\n",
        "        z = circuit(x, W)\n",
        "        p = 0.5 * (1 + z)\n",
        "        outs.append(qnp.clip(p, 1e-7, 1-1e-7))\n",
        "    return qnp.stack(outs)\n",
        "\n",
        "def bce_loss(W, Xb, yb):\n",
        "    p = predict_batch(W, Xb)\n",
        "    return -qnp.mean(yb*qnp.log(p) + (1-yb)*qnp.log(1-p))\n",
        "\n",
        "def make_toy_fraud(n=1200, seed=42):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    n0 = int(n*0.9)\n",
        "    X0 = rng.normal(loc=[0.2,0.4,0.3,0.2], scale=[0.1,0.15,0.15,0.1], size=(n0,4))\n",
        "    y0 = np.zeros(n0)\n",
        "    n1 = n - n0\n",
        "    X1 = rng.normal(loc=[0.7,0.8,0.8,0.7], scale=[0.15,0.15,0.1,0.1], size=(n1,4))\n",
        "    y1 = np.ones(n1)\n",
        "    X = np.vstack([X0, X1])\n",
        "    y = np.concatenate([y0, y1])\n",
        "    X = np.clip(X, 0, 1)\n",
        "    return X, y.astype(int)\n",
        "\n",
        "# Pydantic models\n",
        "class TransactionInput(BaseModel):\n",
        "    amount: float\n",
        "    hour: int\n",
        "    device: str\n",
        "    merchant_risk: float\n",
        "    merchant_category: str\n",
        "    transaction_type: str\n",
        "    cardholder_age: int\n",
        "\n",
        "class TrainingConfig(BaseModel):\n",
        "    epochs: int = 20\n",
        "    batch_size: int = 64\n",
        "    stepsize: float = 0.2\n",
        "    seed: int = 123\n",
        "\n",
        "class PredictionResponse(BaseModel):\n",
        "    quantum_prediction: float\n",
        "    classical_rf_prediction: float\n",
        "    classical_lr_prediction: float\n",
        "    hybrid_prediction: float\n",
        "    is_fraud: bool\n",
        "\n",
        "class TrainingResponse(BaseModel):\n",
        "    success: bool\n",
        "    message: str\n",
        "    metrics: dict\n",
        "\n",
        "class AnalyticsResponse(BaseModel):\n",
        "    model_performance: dict\n",
        "    feature_importance: dict\n",
        "    confusion_matrix: List[List[int]]\n",
        "\n",
        "# API Routes\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    return {\"message\": \"Quantum Fraud Detection API is running\"}\n",
        "\n",
        "@app.get(\"/health\")\n",
        "async def health_check():\n",
        "    return {\"status\": \"healthy\", \"quantum_device\": str(dev)}\n",
        "\n",
        "@app.post(\"/upload-csv\")\n",
        "async def upload_csv(file: UploadFile = File(...)):\n",
        "    try:\n",
        "        content = await file.read()\n",
        "        df = pd.read_csv(io.StringIO(content.decode('utf-8')))\n",
        "\n",
        "        # Validate required columns\n",
        "        required_cols = [\"amount\", \"time\", \"device\", \"merchant_risk\", \"label\",\n",
        "                        \"merchant_category\", \"transaction_type\", \"cardholder_age\"]\n",
        "        missing_cols = [col for col in required_cols if col.lower() not in [c.lower() for c in df.columns]]\n",
        "\n",
        "        if missing_cols:\n",
        "            raise HTTPException(status_code=400, detail=f\"Missing columns: {missing_cols}\")\n",
        "\n",
        "        return {\n",
        "            \"success\": True,\n",
        "            \"message\": f\"CSV uploaded successfully with {len(df)} rows\",\n",
        "            \"preview\": df.head().to_dict('records'),\n",
        "            \"columns\": df.columns.tolist(),\n",
        "            \"shape\": df.shape\n",
        "        }\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=400, detail=str(e))\n",
        "\n",
        "@app.post(\"/train-quantum\", response_model=TrainingResponse)\n",
        "async def train_quantum_model(config: TrainingConfig):\n",
        "    global quantum_model, scaler_quantum\n",
        "\n",
        "    try:\n",
        "        # Generate toy data for training\n",
        "        X, y = make_toy_fraud(n=1400, seed=config.seed)\n",
        "\n",
        "        # Scale features\n",
        "        scaler_quantum = StandardScaler()\n",
        "        X_scaled = scaler_quantum.fit_transform(X)\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X_scaled, y, test_size=0.2, random_state=config.seed, stratify=y\n",
        "        )\n",
        "\n",
        "        # Initialize quantum weights\n",
        "        qnp.random.seed(config.seed)\n",
        "        W = qnp.array(0.01 * qnp.random.randn(2*n_qubits), requires_grad=True)\n",
        "\n",
        "        # Training\n",
        "        opt = qml.GradientDescentOptimizer(stepsize=config.stepsize)\n",
        "        X_train_q = qnp.array(X_train, requires_grad=False)\n",
        "        y_train_q = qnp.array(y_train, requires_grad=False)\n",
        "\n",
        "        for epoch in range(config.epochs):\n",
        "            idx = np.random.choice(len(X_train), min(config.batch_size, len(X_train)), replace=False)\n",
        "            batchX = X_train_q[idx]\n",
        "            batchY = y_train_q[idx]\n",
        "            W = opt.step(lambda w: bce_loss(w, batchX, batchY), W)\n",
        "\n",
        "        # Evaluation\n",
        "        probs = predict_batch(W, qnp.array(X_test))\n",
        "        preds = (probs > 0.5).astype(int)\n",
        "        auc = roc_auc_score(y_test, np.asarray(probs))\n",
        "        f1 = f1_score(y_test, np.asarray(preds))\n",
        "\n",
        "        # Store model\n",
        "        quantum_model = W\n",
        "\n",
        "        metrics = {\n",
        "            \"auc\": float(auc),\n",
        "            \"f1\": float(f1),\n",
        "            \"test_accuracy\": float(np.mean(preds == y_test))\n",
        "        }\n",
        "\n",
        "        return TrainingResponse(\n",
        "            success=True,\n",
        "            message=\"Quantum model trained successfully\",\n",
        "            metrics=metrics\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        return TrainingResponse(\n",
        "            success=False,\n",
        "            message=f\"Training failed: {str(e)}\",\n",
        "            metrics={}\n",
        "        )\n",
        "\n",
        "@app.post(\"/train-classical\", response_model=TrainingResponse)\n",
        "async def train_classical_models():\n",
        "    global classical_models, scaler_classical\n",
        "\n",
        "    try:\n",
        "        # Generate toy data\n",
        "        X, y = make_toy_fraud(n=1400, seed=42)\n",
        "        X_extra = np.zeros((len(X), 3))  # Mock extra features\n",
        "        X_all = np.hstack([X, X_extra])\n",
        "\n",
        "        # Scale features\n",
        "        scaler_classical = StandardScaler()\n",
        "        X_scaled = scaler_classical.fit_transform(X_all)\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
        "        )\n",
        "\n",
        "        # Train models\n",
        "        rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "        lr_model = LogisticRegression(random_state=42)\n",
        "\n",
        "        rf_model.fit(X_train, y_train)\n",
        "        lr_model.fit(X_train, y_train)\n",
        "\n",
        "        # Evaluate\n",
        "        rf_probs = rf_model.predict_proba(X_test)[:, 1]\n",
        "        lr_probs = lr_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "        rf_auc = roc_auc_score(y_test, rf_probs)\n",
        "        lr_auc = roc_auc_score(y_test, lr_probs)\n",
        "\n",
        "        # Store models\n",
        "        classical_models = {\n",
        "            \"random_forest\": rf_model,\n",
        "            \"logistic_regression\": lr_model\n",
        "        }\n",
        "\n",
        "        metrics = {\n",
        "            \"random_forest_auc\": float(rf_auc),\n",
        "            \"logistic_regression_auc\": float(lr_auc)\n",
        "        }\n",
        "\n",
        "        return TrainingResponse(\n",
        "            success=True,\n",
        "            message=\"Classical models trained successfully\",\n",
        "            metrics=metrics\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        return TrainingResponse(\n",
        "            success=False,\n",
        "            message=f\"Training failed: {str(e)}\",\n",
        "            metrics={}\n",
        "        )\n",
        "\n",
        "@app.post(\"/predict\", response_model=PredictionResponse)\n",
        "async def predict_transaction(transaction: TransactionInput):\n",
        "    global quantum_model, scaler_quantum, classical_models, scaler_classical\n",
        "\n",
        "    if quantum_model is None or not classical_models:\n",
        "        raise HTTPException(status_code=400, detail=\"Models not trained. Please train models first.\")\n",
        "\n",
        "    try:\n",
        "        # Map categorical features\n",
        "        device_map = {\"Mobile\": 0.2, \"Desktop\": 0.5, \"ATM\": 0.8}\n",
        "        cat_map = {\"Electronics\": 0.0, \"Grocery\": 0.5, \"Entertainment\": 1.0}\n",
        "        type_map = {\"Online\": 0.2, \"In-Person\": 0.5, \"ATM\": 0.8}\n",
        "\n",
        "        # Prepare quantum features\n",
        "        x_quantum = np.array([\n",
        "            transaction.amount / 1000.0,\n",
        "            transaction.hour / 24.0,\n",
        "            device_map.get(transaction.device, 0.5),\n",
        "            transaction.merchant_risk\n",
        "        ])\n",
        "\n",
        "        # Prepare classical features\n",
        "        x_extra = np.array([\n",
        "            cat_map.get(transaction.merchant_category, 0.5),\n",
        "            type_map.get(transaction.transaction_type, 0.5),\n",
        "            transaction.cardholder_age / 100.0\n",
        "        ])\n",
        "\n",
        "        x_classical = np.hstack([x_quantum, x_extra])\n",
        "\n",
        "        # Scale features\n",
        "        x_quantum_scaled = scaler_quantum.transform([x_quantum])\n",
        "        x_classical_scaled = scaler_classical.transform([x_classical])\n",
        "\n",
        "        # Quantum prediction\n",
        "        quantum_prob = float(predict_batch(quantum_model, qnp.array(x_quantum_scaled))[0])\n",
        "\n",
        "        # Classical predictions\n",
        "        rf_prob = float(classical_models[\"random_forest\"].predict_proba(x_classical_scaled)[:, 1][0])\n",
        "        lr_prob = float(classical_models[\"logistic_regression\"].predict_proba(x_classical_scaled)[:, 1][0])\n",
        "\n",
        "        # Hybrid prediction\n",
        "        hybrid_prob = (quantum_prob + rf_prob) / 2\n",
        "\n",
        "        return PredictionResponse(\n",
        "            quantum_prediction=quantum_prob,\n",
        "            classical_rf_prediction=rf_prob,\n",
        "            classical_lr_prediction=lr_prob,\n",
        "            hybrid_prediction=hybrid_prob,\n",
        "            is_fraud=hybrid_prob > 0.5\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=400, detail=str(e))\n",
        "\n",
        "@app.get(\"/analytics\", response_model=AnalyticsResponse)\n",
        "async def get_analytics():\n",
        "    global quantum_model, classical_models, scaler_quantum, scaler_classical\n",
        "\n",
        "    if quantum_model is None or not classical_models:\n",
        "        raise HTTPException(status_code=400, detail=\"Models not trained. Please train models first.\")\n",
        "\n",
        "    try:\n",
        "        # Generate test data for analytics\n",
        "        X, y = make_toy_fraud(n=500, seed=999)\n",
        "        X_extra = np.zeros((len(X), 3))\n",
        "        X_all = np.hstack([X, X_extra])\n",
        "\n",
        "        # Scale features\n",
        "        X_quantum_scaled = scaler_quantum.transform(X)\n",
        "        X_classical_scaled = scaler_classical.transform(X_all)\n",
        "\n",
        "        # Get predictions\n",
        "        quantum_probs = np.array([float(p) for p in predict_batch(quantum_model, qnp.array(X_quantum_scaled))])\n",
        "        rf_probs = classical_models[\"random_forest\"].predict_proba(X_classical_scaled)[:, 1]\n",
        "        lr_probs = classical_models[\"logistic_regression\"].predict_proba(X_classical_scaled)[:, 1]\n",
        "\n",
        "        # Calculate metrics\n",
        "        quantum_auc = roc_auc_score(y, quantum_probs)\n",
        "        rf_auc = roc_auc_score(y, rf_probs)\n",
        "        lr_auc = roc_auc_score(y, lr_probs)\n",
        "\n",
        "        # Feature importance (mock for quantum)\n",
        "        feature_names = [\"Amount\", \"Time\", \"Device\", \"Merchant Risk\"]\n",
        "        rf_importance = classical_models[\"random_forest\"].feature_importances_[:4]  # First 4 features\n",
        "\n",
        "        # Confusion matrix for quantum model\n",
        "        quantum_preds = (quantum_probs > 0.5).astype(int)\n",
        "        cm = confusion_matrix(y, quantum_preds).tolist()\n",
        "\n",
        "        return AnalyticsResponse(\n",
        "            model_performance={\n",
        "                \"quantum_auc\": float(quantum_auc),\n",
        "                \"random_forest_auc\": float(rf_auc),\n",
        "                \"logistic_regression_auc\": float(lr_auc)\n",
        "            },\n",
        "            feature_importance={\n",
        "                feature_names[i]: float(rf_importance[i]) for i in range(len(feature_names))\n",
        "            },\n",
        "            confusion_matrix=cm\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=400, detail=str(e))\n",
        "\n",
        "@app.post(\"/simulate-transactions\")\n",
        "async def simulate_random_transactions(count: int = 10):\n",
        "    global quantum_model, classical_models, scaler_quantum, scaler_classical\n",
        "\n",
        "    if quantum_model is None or not classical_models:\n",
        "        raise HTTPException(status_code=400, detail=\"Models not trained. Please train models first.\")\n",
        "\n",
        "    try:\n",
        "        results = []\n",
        "        device_map = {\"Mobile\": 0.2, \"Desktop\": 0.5, \"ATM\": 0.8}\n",
        "        cat_map = {\"Electronics\": 0.0, \"Grocery\": 0.5, \"Entertainment\": 1.0}\n",
        "        type_map = {\"Online\": 0.2, \"In-Person\": 0.5, \"ATM\": 0.8}\n",
        "\n",
        "        for i in range(count):\n",
        "            # Generate random transaction\n",
        "            amount = np.random.randint(10, 2000)\n",
        "            hour = np.random.randint(0, 24)\n",
        "            device = np.random.choice([\"Mobile\", \"Desktop\", \"ATM\"])\n",
        "            merchant_risk = np.random.rand()\n",
        "            category = np.random.choice([\"Electronics\", \"Grocery\", \"Entertainment\"])\n",
        "            trans_type = np.random.choice([\"Online\", \"In-Person\", \"ATM\"])\n",
        "            age = np.random.randint(18, 80)\n",
        "\n",
        "            # Prepare features\n",
        "            x_quantum = np.array([\n",
        "                amount / 1000.0, hour / 24.0,\n",
        "                device_map[device], merchant_risk\n",
        "            ])\n",
        "            x_extra = np.array([\n",
        "                cat_map[category], type_map[trans_type], age / 100.0\n",
        "            ])\n",
        "            x_classical = np.hstack([x_quantum, x_extra])\n",
        "\n",
        "            # Scale and predict\n",
        "            x_quantum_scaled = scaler_quantum.transform([x_quantum])\n",
        "            x_classical_scaled = scaler_classical.transform([x_classical])\n",
        "\n",
        "            quantum_prob = float(predict_batch(quantum_model, qnp.array(x_quantum_scaled))[0])\n",
        "            rf_prob = float(classical_models[\"random_forest\"].predict_proba(x_classical_scaled)[:, 1][0])\n",
        "            hybrid_prob = (quantum_prob + rf_prob) / 2\n",
        "\n",
        "            results.append({\n",
        "                \"id\": i + 1,\n",
        "                \"amount\": amount,\n",
        "                \"hour\": hour,\n",
        "                \"device\": device,\n",
        "                \"merchant_risk\": round(merchant_risk, 3),\n",
        "                \"category\": category,\n",
        "                \"transaction_type\": trans_type,\n",
        "                \"age\": age,\n",
        "                \"quantum_prediction\": round(quantum_prob, 3),\n",
        "                \"rf_prediction\": round(rf_prob, 3),\n",
        "                \"hybrid_prediction\": round(hybrid_prob, 3),\n",
        "                \"is_fraud\": hybrid_prob > 0.5\n",
        "            })\n",
        "\n",
        "            # Small delay for realistic simulation\n",
        "            await asyncio.sleep(0.1)\n",
        "\n",
        "        return {\"transactions\": results}\n",
        "\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=400, detail=str(e))\n",
        "\n",
        "@app.get(\"/model-status\")\n",
        "async def get_model_status():\n",
        "    return {\n",
        "        \"quantum_model_trained\": quantum_model is not None,\n",
        "        \"classical_models_trained\": bool(classical_models),\n",
        "        \"available_models\": list(classical_models.keys()) if classical_models else []\n",
        "    }\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Start FastAPI + ngrok in Colab\n",
        "# =========================\n",
        "if __name__ == \"__main__\":\n",
        "    nest_asyncio.apply()\n",
        "\n",
        "    # Start ngrok tunnel\n",
        "    public_url = ngrok.connect(8000)\n",
        "    print(\"ðŸš€ FastAPI is live at:\", public_url)\n",
        "\n",
        "    # Allow CORS for ngrok URL\n",
        "    app.add_middleware(\n",
        "        CORSMiddleware,\n",
        "        allow_origins=[str(public_url), \"http://localhost:3000\", \"http://localhost:5173\"],\n",
        "        allow_credentials=True,\n",
        "        allow_methods=[\"*\"],\n",
        "        allow_headers=[\"*\"],\n",
        "    )\n",
        "\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBfPEhT9OSHB",
        "outputId": "b7aa91a1-62a1-4aa3-e013-e0f934c00ee0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸš€ FastAPI is live at: NgrokTunnel: \"https://6743167504cc.ngrok-free.app\" -> \"http://localhost:8000\"\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [147]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     2409:40f0:410c:41ea:442b:1ba1:c69e:ce19:0 - \"OPTIONS /model-status HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:442b:1ba1:c69e:ce19:0 - \"OPTIONS /model-status HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:442b:1ba1:c69e:ce19:0 - \"GET /model-status HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:442b:1ba1:c69e:ce19:0 - \"GET /model-status HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:442b:1ba1:c69e:ce19:0 - \"OPTIONS /health HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:442b:1ba1:c69e:ce19:0 - \"GET /health HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:442b:1ba1:c69e:ce19:0 - \"OPTIONS /train-quantum HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-195' coro=<Server.serve() done, defined at /usr/local/lib/python3.12/dist-packages/uvicorn/server.py:69> exception=KeyboardInterrupt()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/main.py\", line 580, in run\n",
            "    server.run()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 67, in run\n",
            "    return asyncio.run(self.serve(sockets=sockets))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/nest_asyncio.py\", line 30, in run\n",
            "    return loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/nest_asyncio.py\", line 133, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 396, in __wakeup\n",
            "    self.__step()\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 303, in __step\n",
            "    self.__step_run_and_handle_result(exc)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 70, in serve\n",
            "    with self.capture_signals():\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/contextlib.py\", line 144, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 331, in capture_signals\n",
            "    signal.raise_signal(captured_signal)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     2409:40f0:410c:41ea:442b:1ba1:c69e:ce19:0 - \"POST /train-quantum HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:442b:1ba1:c69e:ce19:0 - \"OPTIONS /train-classical HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:442b:1ba1:c69e:ce19:0 - \"GET /model-status HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:442b:1ba1:c69e:ce19:0 - \"POST /train-classical HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:442b:1ba1:c69e:ce19:0 - \"GET /model-status HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:442b:1ba1:c69e:ce19:0 - \"GET /model-status HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:442b:1ba1:c69e:ce19:0 - \"OPTIONS /simulate-transactions?count=10 HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:442b:1ba1:c69e:ce19:0 - \"POST /simulate-transactions?count=10 HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:442b:1ba1:c69e:ce19:0 - \"OPTIONS /analytics HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:442b:1ba1:c69e:ce19:0 - \"OPTIONS /analytics HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:442b:1ba1:c69e:ce19:0 - \"GET /analytics HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:442b:1ba1:c69e:ce19:0 - \"GET /analytics HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:442b:1ba1:c69e:ce19:0 - \"GET /health HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:442b:1ba1:c69e:ce19:0 - \"GET /health HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:442b:1ba1:c69e:ce19:0 - \"GET /model-status HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:442b:1ba1:c69e:ce19:0 - \"GET /health HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:442b:1ba1:c69e:ce19:0 - \"GET /model-status HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:442b:1ba1:c69e:ce19:0 - \"GET /analytics HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:442b:1ba1:c69e:ce19:0 - \"GET /analytics HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:442b:1ba1:c69e:ce19:0 - \"GET /health HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:442b:1ba1:c69e:ce19:0 - \"GET /health HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:442b:1ba1:c69e:ce19:0 - \"GET /model-status HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:442b:1ba1:c69e:ce19:0 - \"GET /health HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:442b:1ba1:c69e:ce19:0 - \"GET /model-status HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:442b:1ba1:c69e:ce19:0 - \"GET /health HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:442b:1ba1:c69e:ce19:0 - \"GET /model-status HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:442b:1ba1:c69e:ce19:0 - \"GET /health HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:442b:1ba1:c69e:ce19:0 - \"GET /model-status HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:442b:1ba1:c69e:ce19:0 - \"GET /health HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:442b:1ba1:c69e:ce19:0 - \"GET /model-status HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:442b:1ba1:c69e:ce19:0 - \"GET /health HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:442b:1ba1:c69e:ce19:0 - \"GET /model-status HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:442b:1ba1:c69e:ce19:0 - \"GET /health HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:442b:1ba1:c69e:ce19:0 - \"GET /model-status HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:442b:1ba1:c69e:ce19:0 - \"GET /health HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [147]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python app.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1092aFH3I5yx",
        "outputId": "91bcc7d5-20e9-478a-f779-6d476014a292"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/app.py\", line 35, in <module>\n",
            "    clf.fit(X, y)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/ensemble/_forest.py\", line 360, in fit\n",
            "    X, y = validate_data(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\", line 2961, in validate_data\n",
            "    X, y = check_X_y(X, y, **check_params)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\", line 1387, in check_X_y\n",
            "    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\", line 1397, in _check_y\n",
            "    y = check_array(\n",
            "        ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\", line 1107, in check_array\n",
            "    _assert_all_finite(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\", line 120, in _assert_all_finite\n",
            "    _assert_all_finite_element_wise(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\", line 169, in _assert_all_finite_element_wise\n",
            "    raise ValueError(msg_err)\n",
            "ValueError: Input y contains NaN.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DhPeh7PxORT0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}